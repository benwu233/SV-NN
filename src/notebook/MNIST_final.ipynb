{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-PldpnA4COC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74a0b17-ac01-4a2a-ae3c-6dc356dcb934"
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import rpy2.robjects as robjects\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from rpy2.robjects.packages import importr\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "## Install R package BayesGPfit\n",
        "\n",
        "utils = importr('utils')\n",
        "utils.install_packages('BayesGPfit', verbose = 0)\n",
        "GP = importr('BayesGPfit')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/BayesGPfit_0.1.0.tar.gz'\n",
            "\n",
            "R[write to console]: Content type 'application/x-gzip'\n",
            "R[write to console]:  length 26941 bytes (26 KB)\n",
            "\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: downloaded 26 KB\n",
            "\n",
            "\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: \n",
            "R[write to console]: The downloaded source packages are in\n",
            "\t‘/tmp/RtmpzTgHsB/downloaded_packages’\n",
            "R[write to console]: \n",
            "R[write to console]: \n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ-1rGV55FNH"
      },
      "source": [
        "## 1. Model\n",
        "\n",
        "### 1.1  BNNSTGP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV-55ma34w_u"
      },
      "source": [
        "## BNNSTGP Model\n",
        "\n",
        "class BNNSTGP_one_layer(nn.Module):\n",
        "    def __init__(self, input_dim, n_hid, output_dim, w_dim, n_knots, phi, lamb=1.,\n",
        "                 b_prior_sig=1, zeta_prior_sig=1, eta_prior_sig=1, alpha_prior_sig=1,\n",
        "                 act='relu'):\n",
        "        super(BNNSTGP_one_layer, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.n_hid = n_hid\n",
        "        self.output_dim = output_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.n_knots = n_knots\n",
        "        self.phi = phi\n",
        "        self.lamb = lamb\n",
        "\n",
        "        self.sigma = nn.Parameter(torch.tensor(1.))\n",
        "        self.b = nn.Parameter(torch.Tensor(n_knots, n_hid).normal_(0, 1))\n",
        "        self.zeta = nn.Parameter(torch.Tensor(n_hid, output_dim).uniform_(-1, 1))\n",
        "        self.eta = nn.Parameter(torch.Tensor(n_hid).zero_())\n",
        "        self.alpha = nn.Parameter(torch.Tensor(w_dim, output_dim).zero_())\n",
        "\n",
        "        self.b_prior_sig = b_prior_sig\n",
        "        self.zeta_prior_sig = zeta_prior_sig\n",
        "        self.eta_prior_sig = eta_prior_sig\n",
        "        self.alpha_prior_sig = alpha_prior_sig\n",
        "\n",
        "        if act == 'relu':\n",
        "            self.act = torch.relu\n",
        "        elif act == 'tanh':\n",
        "            self.act = torch.tanh\n",
        "        elif act == 'sigmoid':\n",
        "            self.act = torch.sigmoid\n",
        "        else:\n",
        "            raise ValueError('Invalid activation function %s' % act)\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        out = torch.mm(self.phi, self.b)\n",
        "        out = F.threshold(out, self.lamb, self.lamb) - F.threshold(-out, self.lamb, self.lamb)\n",
        "        out = self.sigma * out\n",
        "        out = torch.mm(x, out) + self.eta\n",
        "\n",
        "        out = self.act(out)\n",
        "        out = torch.mm(out, self.zeta)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def log_prior(self):\n",
        "        logprior = 0.5 * (self.b ** 2).sum() / (self.b_prior_sig ** 2)\n",
        "        logprior += 0.5 * (self.zeta ** 2).sum() / (self.zeta_prior_sig ** 2)\n",
        "        logprior += 0.5 * (self.eta ** 2).sum() / (self.eta_prior_sig ** 2)\n",
        "        logprior += 0.5 * (self.alpha ** 2).sum() / (self.alpha_prior_sig ** 2)\n",
        "        return logprior\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af3T-9gU5YBo"
      },
      "source": [
        "### 1.2 SGLD "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0y0WaOQ5dDf"
      },
      "source": [
        "\n",
        "\n",
        "## SGLD (Stochastic Gradient Langevin Dynamics)\n",
        "\n",
        "class SGLD(Optimizer):\n",
        "    def __init__(self, params, lr = required, langevin = True):\n",
        "        self.langevin = langevin\n",
        "        defaults = dict(lr=lr)\n",
        "        super(SGLD, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        loss = None\n",
        "        \n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad\n",
        "                \n",
        "                if self.langevin == True:\n",
        "                    langevin_noise = p.new(p.size()).normal_(mean=0, std=1)/np.sqrt(group['lr'])\n",
        "                    p.add_(0.5*d_p + langevin_noise, alpha = -group['lr'])\n",
        "\n",
        "                else:\n",
        "                    p.add_(0.5*d_p, alpha = -group['lr'])\n",
        "\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAx3l4Nx5iR8"
      },
      "source": [
        "### 1.3 Network Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37tPkgQE5kaG"
      },
      "source": [
        "\n",
        "## Network Wrapper\n",
        "\n",
        "class Net(object):\n",
        "\n",
        "    def __init__(self, task='binary', lr=1e-3, input_dim=784, n_hid = 128, output_dim = 1, w_dim = 1, n_knots = 66,\n",
        "                 N_train=200, phi=None, lamb = 1, langevin = True, step_decay_epoch = 100, step_gamma = 0.1, act = 'relu'):\n",
        "        \n",
        "        #print(' Creating Net!! ')\n",
        "        self.task = task\n",
        "        if task not in ['binary', 'multiclass']:\n",
        "            raise ValueError('Invalid task %s' % task)\n",
        "        self.lr = lr\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.n_hid = n_hid\n",
        "        self.output_dim = output_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.n_knots = n_knots\n",
        "        self.phi = phi\n",
        "        self.lamb = lamb\n",
        "        self.act = act\n",
        "\n",
        "        self.N_train = N_train\n",
        "        self.langevin = langevin\n",
        "        self.step_decay_epoch = step_decay_epoch\n",
        "        self.step_gamma = step_gamma\n",
        "\n",
        "        self.create_net()\n",
        "        self.create_opt()\n",
        "        self.epoch = 0\n",
        "        \n",
        "        self.weight_set_samples = []\n",
        "\n",
        "\n",
        "    def create_net(self):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = BNNSTGP_one_layer(input_dim=self.input_dim, n_hid=self.n_hid, output_dim=self.output_dim, \n",
        "                            w_dim=self.w_dim, n_knots = self.n_knots, phi=torch.tensor(self.phi).to(self.device),\n",
        "                            lamb = self.lamb, act = self.act)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "\n",
        "    def create_opt(self):\n",
        "        self.optimizer = SGLD(params=self.model.parameters(), lr=self.lr, langevin = self.langevin)\n",
        "        self.scheduler = StepLR(self.optimizer, step_size = self.step_decay_epoch, gamma=self.step_gamma)\n",
        "\n",
        "\n",
        "    def fit(self, x, w, y, threshold=0.5):\n",
        "        x = x.to(self.device)\n",
        "        w = w.to(self.device)\n",
        "\n",
        "        if self.task == 'binary':\n",
        "            y = y.float().to(self.device).reshape(-1, 1)\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            out = self.model(x, w)\n",
        "            loss = F.binary_cross_entropy_with_logits(out, y, reduction='mean')\n",
        "            loss = loss * self.N_train \n",
        "            #loss += self.model.log_prior()\n",
        "            \n",
        "            loss.backward()\n",
        "            self.optimizer.step() \n",
        "\n",
        "            pred = (torch.sigmoid(out)>threshold).long()\n",
        "            accu = (pred == y.long()).sum().float()\n",
        "\n",
        "        else:                           ## multiclass\n",
        "            y = y.long().to(self.device).reshape(-1)\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            out = self.model(x, w)\n",
        "            loss = F.cross_entropy(out, y, reduction = 'mean')\n",
        "            loss = loss * self.N_train \n",
        "            loss += self.model.log_prior()\n",
        "            \n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            pred = out.max(dim=1, keepdim=False)[1]\n",
        "            accu = (pred == y).sum()\n",
        "\n",
        "        return loss*x.shape[0]/self.N_train, accu\n",
        "\n",
        "    \n",
        "    def eval(self, x, w, y, threshold=0.5):\n",
        "        x = x.to(self.device)\n",
        "        w = w.to(self.device)\n",
        "        \n",
        "        if self.task == 'binary':\n",
        "            y = y.float().to(self.device).reshape(-1, 1)\n",
        "\n",
        "            out = self.model(x, w)\n",
        "            loss = F.binary_cross_entropy_with_logits(out, y, reduction='mean') \n",
        "            loss = loss * self.N_train \n",
        "            loss += self.model.log_prior()\n",
        "\n",
        "            pred = (torch.sigmoid(out)>threshold).float() \n",
        "            accu = (pred == y).sum().float()\n",
        "\n",
        "        else:                        ## multiclass\n",
        "            y = y.long().to(self.device).reshape(-1)\n",
        "\n",
        "            out = self.model(x, w)\n",
        "            loss = F.cross_entropy(out, y, reduction = 'mean')\n",
        "            loss = loss * self.N_train \n",
        "            loss += self.model.log_prior()\n",
        "\n",
        "            pred = out.max(dim=1, keepdim=False)[1]\n",
        "            accu = (pred == y).sum()\n",
        "\n",
        "        return loss*x.shape[0]/self.N_train, accu\n",
        "    \n",
        "\n",
        "    def get_nb_parameters(self):\n",
        "        return sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "\n",
        "    def save_net_weights(self, max_samples):\n",
        "        \n",
        "        if len(self.weight_set_samples) >= max_samples:\n",
        "            self.weight_set_samples.pop(0)\n",
        "            \n",
        "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
        "        #print(' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples) )\n",
        "\n",
        "\n",
        "    def all_sample_eval(self, x, w, y, threshold=0.5):    \n",
        "        x = x.to(self.device)\n",
        "        w = w.to(self.device)\n",
        "        y = y.float().to(self.device)\n",
        "        \n",
        "        pred = x.new(len(self.weight_set_samples), x.shape[0], self.output_dim)\n",
        "        \n",
        "        for i, weight_dict in enumerate(self.weight_set_samples):\n",
        "            self.model.load_state_dict(weight_dict)\n",
        "            out_i = self.model(x, w)\n",
        "            pred[i] = torch.sigmoid(out_i)\n",
        "            \n",
        "        pred = (pred.mean(0)>threshold).float()\n",
        "        accu = (pred == y).sum().float()\n",
        "\n",
        "        return accu\n",
        "\n",
        "\n",
        "    def save(self, filename):\n",
        "        print('Writting %s\\n' % filename)\n",
        "        torch.save({\n",
        "            'epoch': self.epoch,\n",
        "            'lr': self.lr,\n",
        "            'model': self.model,\n",
        "            'optimizer': self.optimizer,\n",
        "            'scheduler': self.scheduler}, filename)\n",
        "\n",
        "    def load(self, filename):\n",
        "        print('Reading %s\\n' % filename)\n",
        "        state_dict = torch.load(filename)\n",
        "        self.epoch = state_dict['epoch']\n",
        "        self.lr = state_dict['lr']\n",
        "        self.model = state_dict['model']\n",
        "        self.optimizer = state_dict['optimizer']\n",
        "        self.scheduler = state_dict['scheduler']\n",
        "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
        "        return self.epoch\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTCp_4765oXh"
      },
      "source": [
        "## 2. Load MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aaz_uhP5yaw"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.1307,], [0.3081,])\n",
        "])\n",
        "\n",
        "train = torchvision.datasets.MNIST(root = './data/fmnist', train = True, download = True, transform = transform)\n",
        "val = torchvision.datasets.MNIST(root = './data/fmnist', train = False, download = True, transform = transform)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D57lUkGQ49bo"
      },
      "source": [
        "robjects.r['load'](\"./index/mnist_idx.RData\")\n",
        "\n",
        "class mydata(Dataset):\n",
        "    def __init__(self, x_list, y_list):\n",
        "        self.x_list = x_list\n",
        "        self.y_list = y_list\n",
        "    def __len__(self):\n",
        "        return len(self.y_list)\n",
        "    def __getitem__(self, i):\n",
        "        x = self.x_list[i].reshape(-1)\n",
        "        w = torch.tensor([1.])\n",
        "        y = torch.tensor([self.y_list[i]])\n",
        "        return (x, w), y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rv5rQSe59yd"
      },
      "source": [
        "## 3. Training using full data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZK9VcEW6Cv5"
      },
      "source": [
        "\n",
        "def MNIST_all(c1 = 3, c2 = 5, n_epochs = 10000, lr = 1e-5, b = 10, lamb = 2.5, langevin = True, seed = 17, act = 'relu'):\n",
        "    \n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    x_list_c1 = []\n",
        "    y_list_c1 = []\n",
        "    x_list_c2 = []\n",
        "    y_list_c2 = []\n",
        "\n",
        "    for x, y in train:\n",
        "        if y==c1:\n",
        "            x_list_c1.append(x)\n",
        "            y_list_c1.append(0)\n",
        "        if y==c2:\n",
        "            x_list_c2.append(x)\n",
        "            y_list_c2.append(1)\n",
        "\n",
        "    n = 5000\n",
        "    x_list = random.sample(x_list_c1, n) + random.sample(x_list_c2, n)\n",
        "    y_list = random.sample(y_list_c1, n) + random.sample(y_list_c2, n)\n",
        "\n",
        "    MNIST_2digit_train = mydata(x_list, y_list)\n",
        "\n",
        "\n",
        "    ## test set\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for x, y in val:\n",
        "        if y==c1:\n",
        "            x_list.append(x)\n",
        "            y_list.append(0)\n",
        "        if y==c2:\n",
        "            x_list.append(x)\n",
        "            y_list.append(1)\n",
        "\n",
        "    MNIST_2digit_test = mydata(x_list, y_list)\n",
        "\n",
        "    n_train = len(MNIST_2digit_train)\n",
        "    n_test = len(MNIST_2digit_test)\n",
        "\n",
        "    train_batch_size = 128\n",
        "    test_batch_size = 128\n",
        "\n",
        "    train_loader = DataLoader(MNIST_2digit_train, batch_size=train_batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(MNIST_2digit_test, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "    grids = GP.GP_generate_grids(d=2, num_grids=28)\n",
        "    phi = GP.GP_eigen_funcs_fast(grids, b = b, poly_degree = 20)\n",
        "    phi = np.array(phi)\n",
        "\n",
        "    torch.set_default_dtype(torch.float32)\n",
        "\n",
        "    net = Net(lr=lr, input_dim=784, n_hid = 8, output_dim = 1, w_dim = 1, n_knots = phi.shape[1],\n",
        "            N_train=2*n, phi=torch.tensor(phi, dtype=torch.float32), lamb = lamb, langevin=langevin,\n",
        "            step_decay_epoch = 2000, step_gamma = 0.2, act = act)\n",
        "    \n",
        "\n",
        "    epoch = 0\n",
        "\n",
        "    start_save = 3 * n_epochs / 4\n",
        "    save_every = 2\n",
        "    N_saves = 100\n",
        "    test_every = 20\n",
        "    print_every = 100\n",
        "\n",
        "    loss_train = np.zeros(n_epochs)\n",
        "    accu_train = np.zeros(n_epochs)\n",
        "\n",
        "    loss_val = np.zeros(n_epochs)\n",
        "    accu_val = np.zeros(n_epochs)\n",
        "\n",
        "    best_accu = 0\n",
        "\n",
        "    for i in range(epoch, n_epochs):\n",
        "\n",
        "        tic = time.time()\n",
        "        net.scheduler.step()\n",
        "        \n",
        "        for (x, w), y in train_loader:\n",
        "            loss, accu = net.fit(x, w, y)\n",
        "            loss_train[i] += loss\n",
        "            accu_train[i] += accu\n",
        "\n",
        "            \n",
        "        loss_train[i] /= n_train\n",
        "        accu_train[i] /= n_train\n",
        "        toc = time.time()\n",
        "\n",
        "        if i > start_save and i % save_every == 0:\n",
        "            net.save_net_weights(max_samples = N_saves)\n",
        "\n",
        "        \n",
        "        if i % test_every == 0:\n",
        "            with torch.no_grad():\n",
        "                tic = time.time()\n",
        "                for (x, w), y in val_loader:\n",
        "                    loss, accu = net.eval(x, w, y)\n",
        "\n",
        "                    loss_val[i] += loss\n",
        "                    accu_val[i] += accu\n",
        "\n",
        "                loss_val[i] /= n_test\n",
        "                accu_val[i] /= n_test\n",
        "                toc = time.time()\n",
        "                best_accu = max(best_accu, accu_val[i])\n",
        "            \n",
        "            if i % print_every == 0:\n",
        "                print('Epoch %d, train accuracy %.2f%%' % (i, accu_train[i]*100))\n",
        "                print('  Epoch %d, test accuracy %.2f%%' % (i, accu_val[i]*100))\n",
        "        \n",
        "    print('%d vs %d, best test accuracy: %s' % (c1, c2, best_accu))\n",
        "\n",
        "    return net"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP6d5aF19i4M"
      },
      "source": [
        "4 vs 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv4ztcZk6Jtu",
        "outputId": "91347235-3aa2-49cf-85e8-a727b0429d19"
      },
      "source": [
        "net = MNIST_all(c1 = 4, c2 = 7, n_epochs = 5000, lr = 5e-6, b = 10, lamb = 2.5, langevin = True, act = 'relu')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, train accuracy 70.66%\n",
            "  Epoch 0, test accuracy 82.09%\n",
            "Epoch 100, train accuracy 98.21%\n",
            "  Epoch 100, test accuracy 98.31%\n",
            "Epoch 200, train accuracy 98.57%\n",
            "  Epoch 200, test accuracy 98.31%\n",
            "Epoch 300, train accuracy 98.68%\n",
            "  Epoch 300, test accuracy 98.46%\n",
            "Epoch 400, train accuracy 98.68%\n",
            "  Epoch 400, test accuracy 98.51%\n",
            "Epoch 500, train accuracy 98.75%\n",
            "  Epoch 500, test accuracy 98.61%\n",
            "Epoch 600, train accuracy 98.64%\n",
            "  Epoch 600, test accuracy 98.66%\n",
            "Epoch 700, train accuracy 98.78%\n",
            "  Epoch 700, test accuracy 98.51%\n",
            "Epoch 800, train accuracy 98.82%\n",
            "  Epoch 800, test accuracy 98.81%\n",
            "Epoch 900, train accuracy 98.83%\n",
            "  Epoch 900, test accuracy 98.71%\n",
            "Epoch 1000, train accuracy 98.83%\n",
            "  Epoch 1000, test accuracy 98.61%\n",
            "Epoch 1100, train accuracy 98.81%\n",
            "  Epoch 1100, test accuracy 98.86%\n",
            "Epoch 1200, train accuracy 98.91%\n",
            "  Epoch 1200, test accuracy 98.56%\n",
            "Epoch 1300, train accuracy 98.91%\n",
            "  Epoch 1300, test accuracy 98.66%\n",
            "Epoch 1400, train accuracy 98.82%\n",
            "  Epoch 1400, test accuracy 98.86%\n",
            "Epoch 1500, train accuracy 98.91%\n",
            "  Epoch 1500, test accuracy 98.96%\n",
            "Epoch 1600, train accuracy 98.76%\n",
            "  Epoch 1600, test accuracy 98.86%\n",
            "Epoch 1700, train accuracy 98.95%\n",
            "  Epoch 1700, test accuracy 98.81%\n",
            "Epoch 1800, train accuracy 98.95%\n",
            "  Epoch 1800, test accuracy 98.81%\n",
            "Epoch 1900, train accuracy 99.07%\n",
            "  Epoch 1900, test accuracy 98.96%\n",
            "Epoch 2000, train accuracy 99.19%\n",
            "  Epoch 2000, test accuracy 99.05%\n",
            "Epoch 2100, train accuracy 99.12%\n",
            "  Epoch 2100, test accuracy 99.10%\n",
            "Epoch 2200, train accuracy 99.11%\n",
            "  Epoch 2200, test accuracy 99.10%\n",
            "Epoch 2300, train accuracy 99.16%\n",
            "  Epoch 2300, test accuracy 98.86%\n",
            "Epoch 2400, train accuracy 99.13%\n",
            "  Epoch 2400, test accuracy 98.96%\n",
            "Epoch 2500, train accuracy 99.14%\n",
            "  Epoch 2500, test accuracy 99.00%\n",
            "Epoch 2600, train accuracy 99.11%\n",
            "  Epoch 2600, test accuracy 98.91%\n",
            "Epoch 2700, train accuracy 99.12%\n",
            "  Epoch 2700, test accuracy 99.00%\n",
            "Epoch 2800, train accuracy 99.19%\n",
            "  Epoch 2800, test accuracy 99.00%\n",
            "Epoch 2900, train accuracy 99.20%\n",
            "  Epoch 2900, test accuracy 99.00%\n",
            "Epoch 3000, train accuracy 99.19%\n",
            "  Epoch 3000, test accuracy 99.00%\n",
            "Epoch 3100, train accuracy 99.17%\n",
            "  Epoch 3100, test accuracy 98.96%\n",
            "Epoch 3200, train accuracy 99.19%\n",
            "  Epoch 3200, test accuracy 98.96%\n",
            "Epoch 3300, train accuracy 99.21%\n",
            "  Epoch 3300, test accuracy 99.00%\n",
            "Epoch 3400, train accuracy 99.19%\n",
            "  Epoch 3400, test accuracy 98.96%\n",
            "Epoch 3500, train accuracy 99.22%\n",
            "  Epoch 3500, test accuracy 99.05%\n",
            "Epoch 3600, train accuracy 99.30%\n",
            "  Epoch 3600, test accuracy 99.10%\n",
            "Epoch 3700, train accuracy 99.19%\n",
            "  Epoch 3700, test accuracy 99.05%\n",
            "Epoch 3800, train accuracy 99.34%\n",
            "  Epoch 3800, test accuracy 99.00%\n",
            "Epoch 3900, train accuracy 99.30%\n",
            "  Epoch 3900, test accuracy 99.05%\n",
            "Epoch 4000, train accuracy 99.34%\n",
            "  Epoch 4000, test accuracy 99.05%\n",
            "Epoch 4100, train accuracy 99.32%\n",
            "  Epoch 4100, test accuracy 99.05%\n",
            "Epoch 4200, train accuracy 99.32%\n",
            "  Epoch 4200, test accuracy 99.00%\n",
            "Epoch 4300, train accuracy 99.31%\n",
            "  Epoch 4300, test accuracy 99.10%\n",
            "Epoch 4400, train accuracy 99.31%\n",
            "  Epoch 4400, test accuracy 99.05%\n",
            "Epoch 4500, train accuracy 99.29%\n",
            "  Epoch 4500, test accuracy 99.05%\n",
            "Epoch 4600, train accuracy 99.34%\n",
            "  Epoch 4600, test accuracy 99.10%\n",
            "Epoch 4700, train accuracy 99.33%\n",
            "  Epoch 4700, test accuracy 99.05%\n",
            "Epoch 4800, train accuracy 99.32%\n",
            "  Epoch 4800, test accuracy 99.10%\n",
            "Epoch 4900, train accuracy 99.29%\n",
            "  Epoch 4900, test accuracy 99.10%\n",
            "4 vs 7, best test accuracy: 0.9930348258706467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udVS5Crx6KAP"
      },
      "source": [
        "## 4. Training using limited data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQp3OgVur3XW"
      },
      "source": [
        "def MNIST_50(c1 = 3, c2 = 5, size = 10,\n",
        "          b = 10., lamb = 5., poly_degree = 30, \n",
        "          n_hid = 8, lr = 1e-4, n_epochs = 200, test = False, langevin = False, act = 'relu'):\n",
        "\n",
        "   \n",
        "    beta = np.zeros([50, 100, 28*28, 8])\n",
        "\n",
        "    torch.manual_seed(17)\n",
        "    random.seed(17)\n",
        "\n",
        "    accus = []\n",
        "\n",
        "    if size == 10:\n",
        "        k = 0\n",
        "    elif size == 20:\n",
        "        k = 1\n",
        "    elif size == 30:\n",
        "        k = 2\n",
        "    elif size == 40:\n",
        "        k = 3\n",
        "    elif size == 50:\n",
        "        k = 4\n",
        "    else:\n",
        "        raise 'no size'\n",
        "\n",
        "    if c1 == 3 and c2 == 5:\n",
        "        dat = 'idx35'\n",
        "    elif c1 == 3 and c2 == 8:\n",
        "        dat = 'idx38'\n",
        "    elif c1 == 4 and c2 == 7:\n",
        "        dat = 'idx47'\n",
        "    elif c1 == 4 and c2 == 9:\n",
        "        dat = 'idx49'\n",
        "    else:\n",
        "        raise 'no data'\n",
        "\n",
        "\n",
        "    grids = GP.GP_generate_grids(d=2, num_grids=28)\n",
        "    phi = GP.GP_eigen_funcs_fast(grids, b = b, poly_degree = poly_degree)\n",
        "    phi = np.array(phi)\n",
        "\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for x, y in val:\n",
        "        if y==c1:\n",
        "            x_list.append(x)\n",
        "            y_list.append(0)\n",
        "        if y==c2:\n",
        "            x_list.append(x)\n",
        "            y_list.append(1)\n",
        "\n",
        "    MNIST_2_test = mydata(x_list, y_list)\n",
        "\n",
        "\n",
        "    #for id in tqdm(range(100)):\n",
        "    for id in tqdm(range(50)):\n",
        "        if test:\n",
        "            if id != num:\n",
        "                continue\n",
        "\n",
        "        idx = set(np.array(robjects.r[dat][k], dtype = np.int)[11,])\n",
        "\n",
        "        ii = 0\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        for x, y in train:\n",
        "            ii = ii+1\n",
        "            if ii in idx:\n",
        "                x_list.append(x)\n",
        "                if y == c1:\n",
        "                    y_list.append(0)\n",
        "                elif y == c2:\n",
        "                    y_list.append(1)             \n",
        "\n",
        "\n",
        "        MNIST_2_train = mydata(x_list, y_list)\n",
        "\n",
        "        n_train = len(MNIST_2_train)\n",
        "        n_test = len(MNIST_2_test)\n",
        "\n",
        "\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "\n",
        "        train_batch_size = min(128, size*2)\n",
        "        test_batch_size = min(128, size*2)\n",
        "\n",
        "        train_loader = DataLoader(MNIST_2_train, batch_size=train_batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(MNIST_2_test, batch_size=test_batch_size, shuffle=True)\n",
        "            \n",
        "\n",
        "\n",
        "        net = Net(lr=lr, input_dim=784, n_hid = n_hid, output_dim = 1, w_dim = 1, n_knots = phi.shape[1],\n",
        "            N_train=2*size, phi=torch.tensor(phi, dtype=torch.float32), lamb = lamb, langevin=langevin,\n",
        "            step_decay_epoch = 1000, step_gamma = 1, act = act)\n",
        "        \n",
        "\n",
        "        epoch = 0\n",
        "\n",
        "        start_save = 4 * n_epochs/5 \n",
        "        save_every = 2\n",
        "        N_saves = 100\n",
        "\n",
        "        loss_train = np.zeros(n_epochs)\n",
        "        accu_train = np.zeros(n_epochs)\n",
        "\n",
        "        loss_val = np.zeros(n_epochs)\n",
        "        accu_val = np.zeros(n_epochs)\n",
        "\n",
        "\n",
        "        best_test = 0\n",
        "        for i in range(epoch, n_epochs):\n",
        "\n",
        "            \n",
        "            for (x, w), y in train_loader:\n",
        "                loss, accu = net.fit(x, w, y)\n",
        "                net.scheduler.step()\n",
        "                loss_train[i] += loss\n",
        "                accu_train[i] += accu\n",
        "\n",
        "                \n",
        "            loss_train[i] /= n_train\n",
        "            accu_train[i] /= n_train\n",
        "\n",
        "            if i > start_save and i % save_every == 0:\n",
        "                net.save_net_weights(max_samples = N_saves)\n",
        "            if i % 100 == 0:\n",
        "                accu_test = 0\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for (x, w), y in val_loader:\n",
        "                        _, accu = net.eval(x, w, y)\n",
        "                        accu_test += accu\n",
        "                    accu_test /= n_test\n",
        "                    best_test = max(best_test, accu_test)\n",
        "                \n",
        "\n",
        "        accus.append(best_test.detach().cpu().item())\n",
        "        print(\"dataset %d, accuracy= %.4f\" % (id, best_test))\n",
        "        \n",
        "        for j, weight_dict in enumerate(net.weight_set_samples):\n",
        "            net.model.load_state_dict(weight_dict)\n",
        "            tmp = torch.mm(net.model.phi, net.model.b)         \n",
        "            tmp = F.threshold(tmp, net.model.lamb, net.model.lamb) - F.threshold(-tmp, net.model.lamb, net.model.lamb)  \n",
        "            tmp = net.model.sigma * tmp\n",
        "            beta[id,j] = tmp.cpu().detach().numpy()     \n",
        "\n",
        "    print('c1=%d, c2=%d, n = %d, accu = %.4f (%.4f)' %(c1, c2, size, np.mean(accus), np.std(accus)))\n",
        "\n",
        "    return beta\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH7c8XZF9zYu"
      },
      "source": [
        "4 vs 7\n",
        "\n",
        "size of data = 2*10 = 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b6fjQTNs0Ux",
        "outputId": "45d756c4-7070-48ae-f165-96c74de66da3"
      },
      "source": [
        "beta50 = MNIST_50(c1 = 4, c2 = 7, size = 10, b = 10., lamb = 2.5, \n",
        "                  n_hid = 8, lr = 1e-3, n_epochs = 10000, langevin = True, \n",
        "                  act = 'relu')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:43<35:35, 43.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 0, accuracy= 0.9388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 2/50 [01:27<34:54, 43.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 1, accuracy= 0.9433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 3/50 [02:11<34:14, 43.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 2, accuracy= 0.9592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 4/50 [02:55<33:36, 43.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 3, accuracy= 0.9368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 5/50 [03:39<32:52, 43.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 4, accuracy= 0.9716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 6/50 [04:23<32:08, 43.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 5, accuracy= 0.9264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 7/50 [05:06<31:25, 43.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 6, accuracy= 0.9687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 8/50 [05:50<30:43, 43.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 7, accuracy= 0.9597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 9/50 [06:34<30:01, 43.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 8, accuracy= 0.9502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 10/50 [07:18<29:17, 43.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 9, accuracy= 0.9259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 11/50 [08:03<28:35, 43.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 10, accuracy= 0.9682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 12/50 [08:47<27:51, 44.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 11, accuracy= 0.9547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 13/50 [09:31<27:07, 44.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 12, accuracy= 0.9716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 14/50 [10:14<26:23, 43.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 13, accuracy= 0.9632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 15/50 [10:59<25:40, 44.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 14, accuracy= 0.9463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 16/50 [11:43<24:57, 44.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 15, accuracy= 0.9751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 17/50 [12:27<24:15, 44.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 16, accuracy= 0.9687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 18/50 [13:11<23:32, 44.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 17, accuracy= 0.9468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 19/50 [13:55<22:47, 44.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 18, accuracy= 0.9652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 20/50 [14:39<22:03, 44.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 19, accuracy= 0.9612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 21/50 [15:23<21:18, 44.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 20, accuracy= 0.9701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 22/50 [16:07<20:33, 44.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 21, accuracy= 0.9512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 23/50 [16:51<19:49, 44.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 22, accuracy= 0.9189\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 24/50 [17:35<19:04, 44.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 23, accuracy= 0.9527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 25/50 [18:20<18:22, 44.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 24, accuracy= 0.9647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 26/50 [19:03<17:37, 44.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 25, accuracy= 0.9602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 27/50 [19:47<16:51, 43.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 26, accuracy= 0.9672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 28/50 [20:31<16:07, 43.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 27, accuracy= 0.9637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 29/50 [21:15<15:23, 43.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 28, accuracy= 0.9592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 30/50 [21:59<14:40, 44.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 29, accuracy= 0.9692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 31/50 [22:43<13:55, 43.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 30, accuracy= 0.9209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 32/50 [23:27<13:12, 44.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 31, accuracy= 0.9607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 33/50 [24:12<12:28, 44.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 32, accuracy= 0.9627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 34/50 [24:56<11:45, 44.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 33, accuracy= 0.9498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 35/50 [25:40<11:00, 44.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 34, accuracy= 0.9498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 36/50 [26:23<10:15, 43.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 35, accuracy= 0.9736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 74%|███████▍  | 37/50 [27:07<09:31, 43.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 36, accuracy= 0.9388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 38/50 [27:51<08:47, 43.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 37, accuracy= 0.9373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 39/50 [28:36<08:04, 44.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 38, accuracy= 0.9627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 40/50 [29:20<07:20, 44.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 39, accuracy= 0.9542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 41/50 [30:04<06:36, 44.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 40, accuracy= 0.9458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 42/50 [30:48<05:52, 44.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 41, accuracy= 0.9652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 43/50 [31:32<05:08, 44.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 42, accuracy= 0.9443\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 44/50 [32:16<04:24, 44.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 43, accuracy= 0.9642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 45/50 [33:00<03:40, 44.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 44, accuracy= 0.9637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 46/50 [33:45<02:56, 44.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 45, accuracy= 0.9622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 47/50 [34:29<02:12, 44.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 46, accuracy= 0.9050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 48/50 [35:13<01:28, 44.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 47, accuracy= 0.9522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 49/50 [35:57<00:44, 44.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 48, accuracy= 0.9632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [36:41<00:00, 44.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset 49, accuracy= 0.9642\n",
            "c1=4, c2=7, n = 10, accu = 0.9544 (0.0154)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}