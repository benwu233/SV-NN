{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-PldpnA4COC",
    "outputId": "2ec82fc8-72d4-43d5-e233-6456e05b42ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/BayesGPfit_0.1.0.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 26941 bytes (26 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 26 KB\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: \n",
      "R[write to console]: The downloaded source packages are in\n",
      "\t‘/tmp/Rtmp7wmPMl/downloaded_packages’\n",
      "R[write to console]: \n",
      "R[write to console]: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import rpy2.robjects as robjects\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from rpy2.robjects.packages import importr\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Install R package BayesGPfit\n",
    "\n",
    "utils = importr('utils')\n",
    "utils.install_packages('BayesGPfit', verbose = 0)\n",
    "GP = importr('BayesGPfit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ-1rGV55FNH"
   },
   "source": [
    "## 1. Model\n",
    "\n",
    "### 1.1  BNNSTGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gV-55ma34w_u"
   },
   "outputs": [],
   "source": [
    "## BNNSTGP Model\n",
    "\n",
    "class BNNSTGP_two_layer(nn.Module):\n",
    "    def __init__(self, input_dim, n_hid, n_hid2, output_dim, w_dim, n_knots, phi, lamb=1.,\n",
    "                 b_prior_sig=1, zeta_prior_sig=1, eta_prior_sig=1, alpha_prior_sig=1,\n",
    "                 act='relu'):\n",
    "        super(BNNSTGP_two_layer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hid = n_hid\n",
    "        self.n_hid2 = n_hid2\n",
    "        self.output_dim = output_dim\n",
    "        self.w_dim = w_dim\n",
    "        self.n_knots = n_knots\n",
    "        self.phi = phi  \n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.sigma = nn.Parameter(torch.tensor(1.))\n",
    "        self.b = nn.Parameter(torch.Tensor(n_knots, n_hid).normal_(0, 1))\n",
    "        self.zeta = nn.Parameter(torch.Tensor(n_hid2, output_dim).uniform_(-1, 1))\n",
    "        self.eta = nn.Parameter(torch.Tensor(n_hid).zero_())\n",
    "        self.alpha = nn.Parameter(torch.Tensor(w_dim, output_dim).zero_())\n",
    "        self.fc = nn.Linear(n_hid, n_hid2)\n",
    "\n",
    "        self.b_prior_sig = b_prior_sig\n",
    "        self.zeta_prior_sig = zeta_prior_sig\n",
    "        self.eta_prior_sig = eta_prior_sig\n",
    "        self.alpha_prior_sig = alpha_prior_sig\n",
    "\n",
    "        if act == 'relu':\n",
    "            self.act = torch.relu\n",
    "        elif act == 'tanh':\n",
    "            self.act = torch.tanh\n",
    "        elif act == 'sigmoid':\n",
    "            self.act = torch.sigmoid\n",
    "        else:\n",
    "            raise ValueError('Invalid activation function %s' % act)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        out = torch.mm(self.phi, self.b)  ## beta_tilde, (p, n_hid)\n",
    "        out = F.threshold(out, self.lamb, self.lamb) - F.threshold(-out, self.lamb, self.lamb)  ## g_lambda\n",
    "        out = self.sigma * out\n",
    "        out = torch.mm(x, out) + self.eta  ## beta, (N, n_hid)\n",
    "\n",
    "        out = self.act(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.act(out)\n",
    "        out = torch.mm(out, self.zeta)  # + torch.mm(w, self.alpha)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def log_prior(self):\n",
    "        logprior = 0.5 * (self.b ** 2).sum() / (self.b_prior_sig ** 2)\n",
    "        logprior += 0.5 * (self.zeta ** 2).sum() / (self.zeta_prior_sig ** 2)\n",
    "        logprior += 0.5 * (self.eta ** 2).sum() / (self.eta_prior_sig ** 2)\n",
    "        logprior += 0.5 * (self.alpha ** 2).sum() / (self.alpha_prior_sig ** 2)\n",
    "        return logprior\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af3T-9gU5YBo"
   },
   "source": [
    "### 1.2 SGLD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-0y0WaOQ5dDf"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## SGLD (Stochastic Gradient Langevin Dynamics)\n",
    "\n",
    "class SGLD(Optimizer):\n",
    "    def __init__(self, params, lr = required, langevin = True):\n",
    "        self.langevin = langevin\n",
    "        defaults = dict(lr=lr)\n",
    "        super(SGLD, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        loss = None\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                \n",
    "                if self.langevin == True:\n",
    "                    langevin_noise = p.new(p.size()).normal_(mean=0, std=1)/np.sqrt(group['lr'])\n",
    "                    p.add_(0.5*d_p + langevin_noise, alpha = -group['lr'])\n",
    "\n",
    "                else:\n",
    "                    p.add_(0.5*d_p, alpha = -group['lr'])\n",
    "\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAx3l4Nx5iR8"
   },
   "source": [
    "### 1.3 Network Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "37tPkgQE5kaG"
   },
   "outputs": [],
   "source": [
    "\n",
    "## Network Wrapper\n",
    "class Net(object):\n",
    "\n",
    "    def __init__(self, task='binary', lr=1e-3, input_dim=784, n_hid = 128, n_hid2 = 64, output_dim = 1, w_dim = 1, n_knots = 66,\n",
    "                 N_train=200, phi=None, lamb = 1, langevin = True, step_decay_epoch = 100, step_gamma = 0.1, act = 'relu'):\n",
    "\n",
    "        # print(' Creating Net!! ')\n",
    "        self.task = task\n",
    "        if task not in ['binary', 'multiclass']:\n",
    "            raise ValueError('Invalid task %s' % task)\n",
    "        self.lr = lr\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hid = n_hid\n",
    "        self.n_hid2 = n_hid2\n",
    "        self.output_dim = output_dim\n",
    "        self.w_dim = w_dim\n",
    "        self.n_knots = n_knots\n",
    "        self.phi = phi\n",
    "        self.lamb = lamb\n",
    "        self.act = act\n",
    "\n",
    "        self.N_train = N_train\n",
    "        self.langevin = langevin\n",
    "        self.step_decay_epoch = step_decay_epoch\n",
    "        self.step_gamma = step_gamma\n",
    "\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.weight_set_samples = []\n",
    "\n",
    "\n",
    "    def create_net(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = BNNSTGP_two_layer(input_dim=self.input_dim, n_hid=self.n_hid, n_hid2=self.n_hid2, output_dim=self.output_dim,\n",
    "                                       w_dim=self.w_dim, n_knots = self.n_knots, phi=torch.tensor(self.phi).to(self.device),\n",
    "                                       lamb = self.lamb, act = self.act)\n",
    "        self.model.to(self.device)\n",
    "        # print('    Total params: %.2fK' % (self.get_nb_parameters() / 1000.0))\n",
    "\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = SGLD(params=self.model.parameters(), lr=self.lr, langevin = self.langevin)\n",
    "        self.scheduler = StepLR(self.optimizer, step_size = self.step_decay_epoch, gamma=self.step_gamma)\n",
    "\n",
    "\n",
    "    def fit(self, x, w, y, threshold=0.5):\n",
    "        x = x.to(self.device)\n",
    "        w = w.to(self.device)\n",
    "\n",
    "        if self.task == 'binary':\n",
    "            y = y.float().to(self.device).reshape(-1, 1)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            out = self.model(x, w)\n",
    "            loss = F.binary_cross_entropy_with_logits(out, y, reduction='mean')\n",
    "            loss = loss * self.N_train\n",
    "            # loss += self.model.log_prior()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pred = (torch.sigmoid(out ) >threshold).long()\n",
    "            accu = (pred == y.long()).sum().float()\n",
    "\n",
    "        else:                           ## multiclass\n",
    "            y = y.long().to(self.device).reshape(-1)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            out = self.model(x, w)\n",
    "            loss = F.cross_entropy(out, y, reduction = 'mean')\n",
    "            loss = loss * self.N_train\n",
    "            loss += self.model.log_prior()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pred = out.max(dim=1, keepdim=False)[1]\n",
    "            accu = (pred == y).sum()\n",
    "\n",
    "        return loss *x.shape[0 ] /self.N_train, accu\n",
    "\n",
    "\n",
    "    def eval(self, x, w, y, threshold=0.5):\n",
    "        x = x.to(self.device)\n",
    "        w = w.to(self.device)\n",
    "\n",
    "        if self.task == 'binary':\n",
    "            y = y.float().to(self.device).reshape(-1, 1)\n",
    "\n",
    "            out = self.model(x, w)\n",
    "            loss = F.binary_cross_entropy_with_logits(out, y, reduction='mean')\n",
    "            loss = loss * self.N_train\n",
    "            loss += self.model.log_prior()\n",
    "\n",
    "            pred = (torch.sigmoid(out ) >threshold).float()\n",
    "            accu = (pred == y).sum().float()\n",
    "\n",
    "        else:                        ## multiclass\n",
    "            y = y.long().to(self.device).reshape(-1)\n",
    "\n",
    "            out = self.model(x, w)\n",
    "            loss = F.cross_entropy(out, y, reduction = 'mean')\n",
    "            loss = loss * self.N_train\n",
    "            loss += self.model.log_prior()\n",
    "\n",
    "            pred = out.max(dim=1, keepdim=False)[1]\n",
    "            accu = (pred == y).sum()\n",
    "\n",
    "        return loss *x.shape[0 ] /self.N_train, accu\n",
    "\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "\n",
    "    def save_net_weights(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
    "        # print(' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples) )\n",
    "\n",
    "\n",
    "    def all_sample_eval(self, x, w, y, threshold=0.5):\n",
    "        x = x.to(self.device)\n",
    "        w = w.to(self.device)\n",
    "        y = y.float().to(self.device)\n",
    "\n",
    "        pred = x.new(len(self.weight_set_samples), x.shape[0], self.output_dim)\n",
    "\n",
    "        for i, weight_dict in enumerate(self.weight_set_samples):\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out_i = self.model(x, w)\n",
    "            pred[i] = torch.sigmoid(out_i)\n",
    "\n",
    "        pred = (pred.mean(0 ) >threshold).float()\n",
    "        accu = (pred == y).sum().float()\n",
    "\n",
    "        return accu\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        print('Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer,\n",
    "            'scheduler': self.scheduler}, filename)\n",
    "\n",
    "\n",
    "    def load(self, filename):\n",
    "        print('Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        self.scheduler = state_dict['scheduler']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTCp_4765oXh"
   },
   "source": [
    "## 2. Load Fashion MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4aaz_uhP5yaw"
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "f_train = torchvision.datasets.FashionMNIST(root = './data/fmnist', train = True, download = True, transform = transform)\n",
    "f_val = torchvision.datasets.FashionMNIST(root = './data/fmnist', train = False, download = True, transform = transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D57lUkGQ49bo"
   },
   "outputs": [],
   "source": [
    "robjects.r['load'](\"./index/fmnist_idx.RData\")\n",
    "\n",
    "class mydata(Dataset):\n",
    "    def __init__(self, x_list, y_list):\n",
    "        self.x_list = x_list\n",
    "        self.y_list = y_list\n",
    "    def __len__(self):\n",
    "        return len(self.y_list)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.x_list[i].reshape(-1)\n",
    "        w = torch.tensor([1.])\n",
    "        y = torch.tensor([self.y_list[i]])\n",
    "        return (x, w), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rv5rQSe59yd"
   },
   "source": [
    "## 3. Training using full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oZK9VcEW6Cv5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def FMNIST_all(c1=3, c2=5, n_epochs=10000, lr=1e-5, lamb=2.5, b=10, langevin=True, seed=17, act='relu'):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_list_c1 = []\n",
    "    y_list_c1 = []\n",
    "    x_list_c2 = []\n",
    "    y_list_c2 = []\n",
    "\n",
    "    for x, y in f_train:\n",
    "        if y == c1:\n",
    "            x_list_c1.append(x)\n",
    "            y_list_c1.append(0)\n",
    "        if y == c2:\n",
    "            x_list_c2.append(x)\n",
    "            y_list_c2.append(1)\n",
    "\n",
    "    n = 5000\n",
    "    x_list = random.sample(x_list_c1, n) + random.sample(x_list_c2, n)\n",
    "    y_list = random.sample(y_list_c1, n) + random.sample(y_list_c2, n)\n",
    "\n",
    "    FashionMNIST_2_train = mydata(x_list, y_list)\n",
    "\n",
    "    ## test set\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for x, y in f_val:\n",
    "        if y == c1:\n",
    "            x_list.append(x)\n",
    "            y_list.append(0)\n",
    "        if y == c2:\n",
    "            x_list.append(x)\n",
    "            y_list.append(1)\n",
    "\n",
    "    FashionMNIST_2_test = mydata(x_list, y_list)\n",
    "\n",
    "    n_train = len(FashionMNIST_2_train)\n",
    "    n_test = len(FashionMNIST_2_test)\n",
    "\n",
    "    train_batch_size = 128\n",
    "    test_batch_size = 128\n",
    "\n",
    "    train_loader = DataLoader(FashionMNIST_2_train, batch_size=train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(FashionMNIST_2_test, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "    grids = GP.GP_generate_grids(d=2, num_grids=28)\n",
    "    phi = GP.GP_eigen_funcs_fast(grids, b=b, poly_degree=30)\n",
    "    phi = np.array(phi)\n",
    "\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "\n",
    "    net = Net(lr=lr, input_dim=784, n_hid=128, n_hid2=32, output_dim=1, w_dim=1, n_knots=phi.shape[1],\n",
    "              N_train=2 * n, phi=torch.tensor(phi, dtype=torch.float32), lamb=lamb, langevin=langevin,\n",
    "              step_decay_epoch=2000, step_gamma=0.2, act=act)\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    start_save = 3 * n_epochs / 4\n",
    "    save_every = 2\n",
    "    N_saves = 100\n",
    "    test_every = 20\n",
    "    print_every = 100\n",
    "\n",
    "    loss_train = np.zeros(n_epochs)\n",
    "    accu_train = np.zeros(n_epochs)\n",
    "\n",
    "    loss_val = np.zeros(n_epochs)\n",
    "    accu_val = np.zeros(n_epochs)\n",
    "\n",
    "    best_accu = 0\n",
    "\n",
    "\n",
    "    for i in range(epoch, n_epochs):\n",
    "\n",
    "        tic = time.time()\n",
    "        net.scheduler.step()\n",
    "\n",
    "        for (x, w), y in train_loader:\n",
    "            loss, accu = net.fit(x, w, y)\n",
    "            loss_train[i] += loss\n",
    "            accu_train[i] += accu\n",
    "\n",
    "        loss_train[i] /= n_train\n",
    "        accu_train[i] /= n_train\n",
    "        toc = time.time()\n",
    "\n",
    "        if i > start_save and i % save_every == 0:\n",
    "            net.save_net_weights(max_samples=N_saves)\n",
    "\n",
    "\n",
    "\n",
    "        if i % test_every == 0:\n",
    "            with torch.no_grad():\n",
    "                tic = time.time()\n",
    "                for (x, w), y in val_loader:\n",
    "                    loss, accu = net.eval(x, w, y)\n",
    "\n",
    "                    loss_val[i] += loss\n",
    "                    accu_val[i] += accu\n",
    "\n",
    "                loss_val[i] /= n_test\n",
    "                accu_val[i] /= n_test\n",
    "                toc = time.time()\n",
    "                best_accu = max(best_accu, accu_val[i])\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print('Epoch %d, train time %.4f s, train loss %.4f, train accuracy %.2f%%' % (i, toc-tic, loss_train[i], accu_train[i]*100))\n",
    "                print('  Epoch %d, test time %.4f s, test loss %.4f, test accuracy %.2f%%' % (i, toc-tic, loss_val[i], accu_val[i]*100))\n",
    "        \n",
    "    print('%d vs %d, best test accuracy: %s' % (c1, c2, best_accu))\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP6d5aF19i4M"
   },
   "source": [
    "## Sandal (5) vs Sneaker (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tv4ztcZk6Jtu",
    "outputId": "ac59b77d-ec5d-4f6f-fcd6-9f4b57741164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train time 0.0657 s, train loss 1.7370, train accuracy 73.57%\n",
      "  Epoch 0, test time 0.0657 s, test loss 3.5872, test accuracy 76.75%\n",
      "Epoch 100, train time 0.0620 s, train loss 0.1068, train accuracy 96.27%\n",
      "  Epoch 100, test time 0.0620 s, test loss 3.3088, test accuracy 96.45%\n",
      "Epoch 200, train time 0.0618 s, train loss 0.1094, train accuracy 96.08%\n",
      "  Epoch 200, test time 0.0618 s, test loss 3.4229, test accuracy 92.30%\n",
      "Epoch 300, train time 0.0608 s, train loss 0.0754, train accuracy 97.29%\n",
      "  Epoch 300, test time 0.0608 s, test loss 3.3403, test accuracy 96.80%\n",
      "Epoch 400, train time 0.0609 s, train loss 0.0727, train accuracy 97.34%\n",
      "  Epoch 400, test time 0.0609 s, test loss 3.3867, test accuracy 95.70%\n",
      "Epoch 500, train time 0.0620 s, train loss 0.0662, train accuracy 97.62%\n",
      "  Epoch 500, test time 0.0620 s, test loss 3.3953, test accuracy 96.45%\n",
      "Epoch 600, train time 0.0587 s, train loss 0.0653, train accuracy 97.69%\n",
      "  Epoch 600, test time 0.0587 s, test loss 3.4153, test accuracy 96.60%\n",
      "Epoch 700, train time 0.0602 s, train loss 0.0642, train accuracy 97.74%\n",
      "  Epoch 700, test time 0.0602 s, test loss 3.4362, test accuracy 96.90%\n",
      "Epoch 800, train time 0.0594 s, train loss 0.0600, train accuracy 97.89%\n",
      "  Epoch 800, test time 0.0594 s, test loss 3.4742, test accuracy 96.75%\n",
      "Epoch 900, train time 0.0619 s, train loss 0.0584, train accuracy 97.96%\n",
      "  Epoch 900, test time 0.0619 s, test loss 3.4885, test accuracy 96.75%\n",
      "Epoch 1000, train time 0.0605 s, train loss 0.0597, train accuracy 97.87%\n",
      "  Epoch 1000, test time 0.0605 s, test loss 3.5074, test accuracy 97.30%\n",
      "Epoch 1100, train time 0.0630 s, train loss 0.0568, train accuracy 97.97%\n",
      "  Epoch 1100, test time 0.0630 s, test loss 3.5348, test accuracy 97.10%\n",
      "Epoch 1200, train time 0.0599 s, train loss 0.0516, train accuracy 98.06%\n",
      "  Epoch 1200, test time 0.0599 s, test loss 3.5598, test accuracy 96.75%\n",
      "Epoch 1300, train time 0.0621 s, train loss 0.0484, train accuracy 98.28%\n",
      "  Epoch 1300, test time 0.0621 s, test loss 3.5858, test accuracy 97.10%\n",
      "Epoch 1400, train time 0.0615 s, train loss 0.0477, train accuracy 98.38%\n",
      "  Epoch 1400, test time 0.0615 s, test loss 3.6168, test accuracy 97.00%\n",
      "Epoch 1500, train time 0.0576 s, train loss 0.0439, train accuracy 98.45%\n",
      "  Epoch 1500, test time 0.0576 s, test loss 3.6392, test accuracy 96.80%\n",
      "Epoch 1600, train time 0.0606 s, train loss 0.0455, train accuracy 98.37%\n",
      "  Epoch 1600, test time 0.0606 s, test loss 3.6574, test accuracy 97.40%\n",
      "Epoch 1700, train time 0.0597 s, train loss 0.0450, train accuracy 98.27%\n",
      "  Epoch 1700, test time 0.0597 s, test loss 3.7150, test accuracy 96.35%\n",
      "Epoch 1800, train time 0.0593 s, train loss 0.0473, train accuracy 98.06%\n",
      "  Epoch 1800, test time 0.0593 s, test loss 3.7140, test accuracy 96.90%\n",
      "Epoch 1900, train time 0.0583 s, train loss 0.0465, train accuracy 98.10%\n",
      "  Epoch 1900, test time 0.0583 s, test loss 3.7490, test accuracy 97.00%\n",
      "Epoch 2000, train time 0.0575 s, train loss 0.0402, train accuracy 98.44%\n",
      "  Epoch 2000, test time 0.0575 s, test loss 3.7673, test accuracy 97.35%\n",
      "Epoch 2100, train time 0.0554 s, train loss 0.0407, train accuracy 98.41%\n",
      "  Epoch 2100, test time 0.0554 s, test loss 3.7724, test accuracy 97.05%\n",
      "Epoch 2200, train time 0.0607 s, train loss 0.0409, train accuracy 98.44%\n",
      "  Epoch 2200, test time 0.0607 s, test loss 3.7782, test accuracy 97.35%\n",
      "Epoch 2300, train time 0.0614 s, train loss 0.0424, train accuracy 98.30%\n",
      "  Epoch 2300, test time 0.0614 s, test loss 3.7837, test accuracy 97.35%\n",
      "Epoch 2400, train time 0.0585 s, train loss 0.0407, train accuracy 98.46%\n",
      "  Epoch 2400, test time 0.0585 s, test loss 3.7904, test accuracy 97.15%\n",
      "Epoch 2500, train time 0.0583 s, train loss 0.0413, train accuracy 98.48%\n",
      "  Epoch 2500, test time 0.0583 s, test loss 3.8009, test accuracy 97.30%\n",
      "Epoch 2600, train time 0.0587 s, train loss 0.0400, train accuracy 98.56%\n",
      "  Epoch 2600, test time 0.0587 s, test loss 3.8052, test accuracy 97.05%\n",
      "Epoch 2700, train time 0.0611 s, train loss 0.0403, train accuracy 98.53%\n",
      "  Epoch 2700, test time 0.0611 s, test loss 3.8105, test accuracy 96.95%\n",
      "Epoch 2800, train time 0.0581 s, train loss 0.0381, train accuracy 98.55%\n",
      "  Epoch 2800, test time 0.0581 s, test loss 3.8201, test accuracy 97.00%\n",
      "Epoch 2900, train time 0.0610 s, train loss 0.0381, train accuracy 98.58%\n",
      "  Epoch 2900, test time 0.0610 s, test loss 3.8213, test accuracy 97.05%\n",
      "Epoch 3000, train time 0.0605 s, train loss 0.0390, train accuracy 98.63%\n",
      "  Epoch 3000, test time 0.0605 s, test loss 3.8299, test accuracy 97.10%\n",
      "Epoch 3100, train time 0.0557 s, train loss 0.0384, train accuracy 98.56%\n",
      "  Epoch 3100, test time 0.0557 s, test loss 3.8278, test accuracy 97.20%\n",
      "Epoch 3200, train time 0.0567 s, train loss 0.0377, train accuracy 98.53%\n",
      "  Epoch 3200, test time 0.0567 s, test loss 3.8329, test accuracy 97.15%\n",
      "Epoch 3300, train time 0.0561 s, train loss 0.0360, train accuracy 98.66%\n",
      "  Epoch 3300, test time 0.0561 s, test loss 3.8354, test accuracy 97.35%\n",
      "Epoch 3400, train time 0.0599 s, train loss 0.0354, train accuracy 98.59%\n",
      "  Epoch 3400, test time 0.0599 s, test loss 3.8430, test accuracy 96.95%\n",
      "Epoch 3500, train time 0.0550 s, train loss 0.0342, train accuracy 98.53%\n",
      "  Epoch 3500, test time 0.0550 s, test loss 3.8490, test accuracy 97.05%\n",
      "Epoch 3600, train time 0.0602 s, train loss 0.0347, train accuracy 98.65%\n",
      "  Epoch 3600, test time 0.0602 s, test loss 3.8542, test accuracy 97.20%\n",
      "Epoch 3700, train time 0.0562 s, train loss 0.0354, train accuracy 98.55%\n",
      "  Epoch 3700, test time 0.0562 s, test loss 3.8602, test accuracy 97.35%\n",
      "Epoch 3800, train time 0.0583 s, train loss 0.0351, train accuracy 98.58%\n",
      "  Epoch 3800, test time 0.0583 s, test loss 3.8682, test accuracy 97.60%\n",
      "Epoch 3900, train time 0.0610 s, train loss 0.0358, train accuracy 98.57%\n",
      "  Epoch 3900, test time 0.0610 s, test loss 3.8800, test accuracy 97.20%\n",
      "Epoch 4000, train time 0.0602 s, train loss 0.0342, train accuracy 98.70%\n",
      "  Epoch 4000, test time 0.0602 s, test loss 3.8799, test accuracy 97.40%\n",
      "Epoch 4100, train time 0.0601 s, train loss 0.0346, train accuracy 98.61%\n",
      "  Epoch 4100, test time 0.0601 s, test loss 3.8834, test accuracy 97.30%\n",
      "Epoch 4200, train time 0.0590 s, train loss 0.0340, train accuracy 98.72%\n",
      "  Epoch 4200, test time 0.0590 s, test loss 3.8780, test accuracy 97.55%\n",
      "Epoch 4300, train time 0.0585 s, train loss 0.0345, train accuracy 98.70%\n",
      "  Epoch 4300, test time 0.0585 s, test loss 3.8831, test accuracy 97.10%\n",
      "Epoch 4400, train time 0.0586 s, train loss 0.0347, train accuracy 98.65%\n",
      "  Epoch 4400, test time 0.0586 s, test loss 3.8866, test accuracy 97.25%\n",
      "Epoch 4500, train time 0.0605 s, train loss 0.0344, train accuracy 98.80%\n",
      "  Epoch 4500, test time 0.0605 s, test loss 3.8869, test accuracy 97.45%\n",
      "Epoch 4600, train time 0.0616 s, train loss 0.0345, train accuracy 98.64%\n",
      "  Epoch 4600, test time 0.0616 s, test loss 3.8968, test accuracy 97.25%\n",
      "Epoch 4700, train time 0.0615 s, train loss 0.0342, train accuracy 98.64%\n",
      "  Epoch 4700, test time 0.0615 s, test loss 3.8930, test accuracy 97.20%\n",
      "Epoch 4800, train time 0.0578 s, train loss 0.0349, train accuracy 98.70%\n",
      "  Epoch 4800, test time 0.0578 s, test loss 3.8919, test accuracy 97.15%\n",
      "Epoch 4900, train time 0.0590 s, train loss 0.0351, train accuracy 98.79%\n",
      "  Epoch 4900, test time 0.0590 s, test loss 3.8931, test accuracy 97.25%\n",
      "5 vs 7, best test accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "net = FMNIST_all(c1=5, c2=7, n_epochs = 5000, lr = 1e-6, b = 10, lamb = 2.5, langevin = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udVS5Crx6KAP"
   },
   "source": [
    "## 4. Training using limited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sQp3OgVur3XW"
   },
   "outputs": [],
   "source": [
    "\n",
    "def FMNIST_50(c1=5, c2=7, size=10,\n",
    "              b=10., lamb=5., poly_degree=30,\n",
    "              n_hid=128, n_hid2=64, lr=1e-4, n_epochs=200, langevin=False):\n",
    "\n",
    "    beta = np.zeros([50, 100, 28 * 28, 128])\n",
    "\n",
    "    torch.manual_seed(17)\n",
    "    random.seed(17)\n",
    "\n",
    "    accus = []\n",
    "\n",
    "    if size == 10:\n",
    "        k = 0\n",
    "    elif size == 20:\n",
    "        k = 1\n",
    "    elif size == 30:\n",
    "        k = 2\n",
    "    elif size == 40:\n",
    "        k = 3\n",
    "    elif size == 50:\n",
    "        k = 4\n",
    "    else:\n",
    "        raise ValueError('Invalid Size')\n",
    "\n",
    "    if c1 == 0 and c2 == 2:\n",
    "        dat = 'idx02'\n",
    "    elif c1 == 0 and c2 == 6:\n",
    "        dat = 'idx06'\n",
    "    elif c1 == 5 and c2 == 7:\n",
    "        dat = 'idx57'\n",
    "    elif c1 == 7 and c2 == 9:\n",
    "        dat = 'idx79'\n",
    "    else:\n",
    "        raise ValueError('Invalid Pair')\n",
    "\n",
    "    grids = GP.GP_generate_grids(d=2, num_grids=28)\n",
    "    phi = GP.GP_eigen_funcs_fast(grids, b=b, poly_degree=poly_degree)\n",
    "    phi = np.array(phi)\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for x, y in f_val:\n",
    "        if y == c1:\n",
    "            x_list.append(x)\n",
    "            y_list.append(0)\n",
    "        if y == c2:\n",
    "            x_list.append(x)\n",
    "            y_list.append(1)\n",
    "\n",
    "    FashionMNIST_2_test = mydata(x_list, y_list)\n",
    "\n",
    "    # for id in tqdm(range(100)):\n",
    "    for id in tqdm(range(50)):\n",
    "        idx = set(np.array(robjects.r[dat][k], dtype=np.int)[10,])\n",
    "\n",
    "        ii = 0\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        for x, y in f_train:\n",
    "            ii = ii + 1\n",
    "            if ii in idx:\n",
    "                x_list.append(x)\n",
    "                if y == c1:\n",
    "                    y_list.append(0)\n",
    "                elif y == c2:\n",
    "                    y_list.append(1)\n",
    "\n",
    "        FashionMNIST_2_train = mydata(x_list, y_list)\n",
    "\n",
    "        n_train = len(FashionMNIST_2_train)\n",
    "        n_test = len(FashionMNIST_2_test)\n",
    "\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        train_batch_size = min(128, size * 2)\n",
    "        test_batch_size = min(128, size * 2)\n",
    "\n",
    "        train_loader = DataLoader(FashionMNIST_2_train, batch_size=train_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(FashionMNIST_2_test, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "        net = Net(lr=lr, input_dim=784, n_hid=n_hid, n_hid2=n_hid2, output_dim=1, w_dim=1, n_knots=phi.shape[1],\n",
    "                  N_train=2 * size, phi=torch.tensor(phi, dtype=torch.float32), lamb=lamb, langevin=langevin,\n",
    "                  step_decay_epoch=1000, step_gamma=1)\n",
    "\n",
    "        epoch = 0\n",
    "\n",
    "        start_save = 4 * n_epochs / 5\n",
    "        save_every = 2\n",
    "        N_saves = 100\n",
    "\n",
    "\n",
    "        loss_train = np.zeros(n_epochs)\n",
    "        accu_train = np.zeros(n_epochs)\n",
    "\n",
    "        best_test = 0\n",
    "        for i in range(epoch, n_epochs):\n",
    "\n",
    "            for (x, w), y in train_loader:\n",
    "                loss, accu = net.fit(x, w, y)\n",
    "                net.scheduler.step()\n",
    "                loss_train[i] += loss\n",
    "                accu_train[i] += accu\n",
    "\n",
    "\n",
    "            loss_train[i] /= n_train\n",
    "            accu_train[i] /= n_train\n",
    "\n",
    "            if i > start_save and i % save_every == 0:\n",
    "                net.save_net_weights(max_samples=N_saves)\n",
    "            if i % 100 == 0:\n",
    "                accu_test = 0\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for (x, w), y in val_loader:\n",
    "                        _, accu = net.eval(x, w, y)\n",
    "                        accu_test += accu\n",
    "                    accu_test /= n_test\n",
    "                    best_test = max(best_test, accu_test)\n",
    "\n",
    "        accus.append(best_test.detach().cpu().item())\n",
    "        print(\"dataset %d, accuracy= %.4f\" % (id, best_test))\n",
    "\n",
    "\n",
    "        for j, weight_dict in enumerate(net.weight_set_samples):\n",
    "            net.model.load_state_dict(weight_dict)\n",
    "            tmp = torch.mm(net.model.phi, net.model.b)\n",
    "            tmp = F.threshold(tmp, net.model.lamb, net.model.lamb) - F.threshold(-tmp, net.model.lamb, net.model.lamb)\n",
    "            tmp = net.model.sigma * tmp\n",
    "            beta[id, j] = tmp.cpu().detach().numpy()\n",
    "\n",
    "    print('c1=%d, c2=%d, n = %d, accu = %.4f (%.4f)' %(c1, c2, size, np.mean(accus), np.std(accus)))\n",
    "\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH7c8XZF9zYu"
   },
   "source": [
    "Sandal (5) vs Sneaker (7)\n",
    "\n",
    "size of data = 2*10 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b6fjQTNs0Ux",
    "outputId": "de51b65a-6f94-40d5-c80c-504533445dba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [01:02<50:58, 62.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 0, accuracy= 0.8165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/50 [02:04<49:53, 62.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1, accuracy= 0.8185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 3/50 [03:06<48:46, 62.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 2, accuracy= 0.8095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 4/50 [04:09<47:48, 62.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 3, accuracy= 0.8190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 5/50 [05:11<46:46, 62.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 4, accuracy= 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 6/50 [06:13<45:41, 62.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 5, accuracy= 0.8320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 7/50 [07:14<44:22, 61.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 6, accuracy= 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 8/50 [08:15<43:00, 61.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 7, accuracy= 0.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 9/50 [09:16<41:56, 61.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 8, accuracy= 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 10/50 [10:17<40:48, 61.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 9, accuracy= 0.8805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 11/50 [11:18<39:50, 61.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 10, accuracy= 0.8405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 12/50 [12:19<38:38, 61.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 11, accuracy= 0.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 13/50 [13:19<37:33, 60.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 12, accuracy= 0.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 14/50 [14:20<36:34, 60.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 13, accuracy= 0.8835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 15/50 [15:21<35:34, 60.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 14, accuracy= 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 16/50 [16:22<34:27, 60.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 15, accuracy= 0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 17/50 [17:22<33:23, 60.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 16, accuracy= 0.8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 18/50 [18:23<32:21, 60.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 17, accuracy= 0.8720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 19/50 [19:24<31:20, 60.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 18, accuracy= 0.8105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 20/50 [20:24<30:17, 60.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 19, accuracy= 0.8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 21/50 [21:24<29:14, 60.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 20, accuracy= 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 22/50 [22:25<28:13, 60.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 21, accuracy= 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 23/50 [23:26<27:15, 60.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 22, accuracy= 0.8635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 24/50 [24:26<26:17, 60.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 23, accuracy= 0.8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 25/50 [25:26<25:12, 60.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 24, accuracy= 0.8420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 26/50 [26:26<24:07, 60.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 25, accuracy= 0.8690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 27/50 [27:27<23:06, 60.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 26, accuracy= 0.7920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 28/50 [28:26<22:03, 60.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 27, accuracy= 0.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 29/50 [29:27<21:07, 60.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 28, accuracy= 0.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 30/50 [30:28<20:08, 60.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 29, accuracy= 0.8555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 31/50 [31:28<19:08, 60.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 30, accuracy= 0.8115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 32/50 [32:29<18:07, 60.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 31, accuracy= 0.8535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 33/50 [33:29<17:07, 60.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 32, accuracy= 0.8430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 34/50 [34:30<16:09, 60.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 33, accuracy= 0.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 35/50 [35:31<15:08, 60.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 34, accuracy= 0.8395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 36/50 [36:31<14:05, 60.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 35, accuracy= 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 37/50 [37:31<13:03, 60.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 36, accuracy= 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 38/50 [38:30<12:01, 60.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 37, accuracy= 0.8690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 39/50 [39:31<11:01, 60.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 38, accuracy= 0.8510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 40/50 [40:30<10:00, 60.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 39, accuracy= 0.8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 41/50 [41:31<09:00, 60.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 40, accuracy= 0.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 42/50 [42:31<08:00, 60.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 41, accuracy= 0.8615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 43/50 [43:30<06:59, 59.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 42, accuracy= 0.8670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 44/50 [44:31<06:01, 60.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 43, accuracy= 0.8635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 45/50 [45:31<05:00, 60.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 44, accuracy= 0.8220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 46/50 [46:32<04:00, 60.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 45, accuracy= 0.8330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 47/50 [47:32<03:00, 60.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 46, accuracy= 0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 48/50 [48:32<02:00, 60.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 47, accuracy= 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 49/50 [49:32<01:00, 60.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 48, accuracy= 0.8440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [50:32<00:00, 60.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 49, accuracy= 0.8300\n",
      "c1=5, c2=7, n = 10, accu = 0.8427 (0.0234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "beta50 = FMNIST_50(c1 = 5, c2 = 7, size = 10,\n",
    "      b = 10, lamb = 2.5, poly_degree = 30, \n",
    "      n_hid = 128, n_hid2 = 64, lr = 1e-4, n_epochs = 10000, langevin = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Fashion_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
